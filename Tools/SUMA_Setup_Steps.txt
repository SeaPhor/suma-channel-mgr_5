SUSE Manager Set-Up
[Items in [] are informational only, Or reference a script Option]
[Items in <> are environmental or specific to a variable]

VM-Setup [I use VirtualBox]
- HDD => 250GB [Mine, need at least 125GB]
- RAM => 8GB [Minimum]
- CPU => 2/2 [4 cores]

Install SLES12-SP3 [Configure during DVD/ISO installation]
- SUSE Manager Registration - Register using SUMA Registration Code
- Add-On Modules - Select SUSE Manager 3 Server Module Add-On
- Add-On Modules - Select Adv. System Management Module Add-On
- Add-On Modules - Select Web & Scripting Module Add-On
- Partitioning - Select Edit Proposed and UN-Check Separate /home
- Software - Select GCC-C++
- Firewall - Enable SSH

Initial System Configuration
- Yast2 => System => Network Settings
- - Overview - Edit network card => Static => Set IP, netmask, & fqdn => Next
- - Hostname/DNS - Set Hostname, Domain Name, Nameserver/s IP, & Domain Search
- - Routing - Set IP of Default Gateway IP Address & Interface (eth0)
- - OK
- Verify Network/Internet connectivity
- Patch [if needed] and reboot [if needed] 
- - [I shutdown to get out of the Vbox console and start it with ‘VboxManage startvm <vmname> --type headless’ and then ssh to it from my box]
- Set FQDN & alias in /etc/hosts
- Yast2 ntp-client => configure NTP to Local/Public NTP-Server
- - Run ntpdate -u <ntp-server-fqdn>

Inital SUSE Manager Setup
- Run yast2-susemanager-setup
- - For SCC Credentials section use the ID & Password from https://scc.suse.com/subscriptions => Organization => Organizational Credentials

Initial SUSE Manager Configuration
- Open in a browser https://<SUMA-FQDN> [The WebUI]
- Create Admin User
- SCC Channel Config
- - Admin => Setup Wizard => SUSE Products => Select the "+" to the right of the Product/s
- - - [This will take quite some time to sync with SCC Channels/Repos]

More SUSE Manager Configuration in The WebUI
- Create Additional Users & Roles [as needed]
- Create & configure System Groups [as needed]
- NOT USED - Clone channels in WebUI
- NOT USED - Clone channels in CLI w/spacecmd
- If using my LifeCycle Patch Management script-
- - 'zypper in git' [This may require adding additional suse repository]
- - - ‘zypper ar https://download.opensuse.org/repositories/devel:/tools:/scm/SLE_12_SP3/ git’
- - See "- Registration - MyScript" below
- - - Clone & Promote SUSE Channel Trees w/ MyScript
- - - - Create Local repo/s & channel/s
- - - - - Create any additional repositories for custom rpms, packages, or external repos
- - - - - Create Channel/s for additional repositories, with 'Parent' being the dev channel and named so
- Create & configure activation key/s ['creation' not used if MyScript]
- - Edit Activation keys to include 'Configuration File Deployment', All child channels needed including the custom created channel/s for repositories.

- Registration - Salt Minion [EDITING- will add the commands]
- - Create bootstrap repository/s 
- - Add repo created above to the client-
- - - EXAMPLE => zypper ar -f http://newsuma3.woodbeeco.com/pub/repositories/sle/12/3/bootstrap bootstraprepo
- - - zypper in salt-minion
- - - Create the 'grains' file to /etc/salt/. with the structured parameters/field-values
- - - Edit the /etc/salt/minions to add the suma-server FQDN
- - - systemctl start salt-minion.service
- - - systemctl enable salt-minion.service
-_-_- Some Clients require issuing 'salt-minion' once to sent request to Master, and then restart the minion after accepting the key/s
- - # On SUMA-Server
- - - # Salt => Keys => Accept
- - - OR BEFORE Client bootstraps do:
- - - Edit /etc/salt/master and set "auto_accept: True"
- - - - 'systemctl restart salt-master.service'

- Registration - Traditional
- - Create bootstrap repository/s [not used if MyScript]
- - Create bootstrap script/s [not used if MyScript]
- - - mgr-bootstrap
- - - cp /srv/www/htdocs/pub/bootstrap/bootstrap.sh /srv/www/htdocs/pub/bootstrap/newname-bootstrap.sh
- - - Edit newname-bootstrap.sh and make the value of ACTIVATION_KEYS= the same as the matching OS/SP actvation key/s created ealier
- - - - wget http://sumaserver.domain.com/pub/bootstrap/newname-bootstrap.sh
- - - - chmod +x newname-bootstrap.sh
- - - - sh newname-bootstrap.sh

- Registration – MyScript [EDITING- will add the commands]
- - Download MyScript with 'git clone https://github.com/seaphor-wbc/SUSEManager.git' (may need to make it executable) 
- - - 'ln -s ~/SUSEManager/Latest_Stable/channellock-promote.sh ~/bin/channellock-promote.sh
- - channellock-promote.sh # to see options and required parameters
- - channellock-promote.sh -g # to see GNU/GPL Info (and disclaimer)
- - channellock-promote.sh -b # to begin the initial run
- - - Provide the details for the prompts on the initial run to:
- - - - Install the required dependency/s
- - - - Create the credentials file
- - - - Set the authentication for the 'mgr-sync refresh' command into ~/.mgr-sync
- - - On Client Traditional
- - - - wget http://sumaserver.domain.com/pub/bootstrap/dev-sle-<osv>-<ossp>-x86_64-bootstrap.sh
- - - - chmod +x *bootstrap.sh
- - - - sh dev-sle-<osv>-<ossp>-x86_64-bootstrap.sh
- - - On Client Salt
- - - - Follow the “Registration - Salt Minion” above
- - Create 'test' & 'prod' channels to assign to corresponding clients' environment.
- - - channellock-promote.sh # to see options and required parameters
- - - channellock-promote.sh -b # to clone the latest packages in the SUSE Channel/s Tree/s to 'dev'
- - - channellock-promote.sh -d # to promote the entire 'dev' tree/s to 'test'
- - - channellock-promote.sh -p # to promote the entire 'test' tree/s to 'prod'
- - To assign clients to 'test' or 'prod':
- - - On Client Traditional
- - - - wget http://sumaserver.domain.com/pub/bootstrap/<'test||prod'>-sle-<osv>-<ossp>-x86_64-bootstrap.sh
- - - - chmod +x *bootstrap.sh
- - - - sh <'test||prod'>-sle-<osv>-<ossp>-x86_64-bootstrap.sh
- - channellock-promote.sh -g # to see the GPL Information

Initial Setup Configuration Completed
- Use the above chosen "Registration... => On Client" method to register all/new clients

See APPENDIX-B at bottom of this document for more steps with Salt
-

Local repo/s & channel/s
- I have used 2 different methods for adding a local rpm repository to a channel, each have their own benefits, I will list them both but will only detail the most efficient way.
- Create a repo in '/srv/www/htdocs/pub/repositories/' and either use my script's ".creds.sh" file to add a function to add it to given channel/s, or do it manually with the "spacecmd softwarechannel_addrepo" command
- - This method works and is functional, but, has to be re-added every time the channels are cloned or re-sync'd.
- Create a repo in the WebUI and a Channel for it.
- - Create a repo in '/srv/www/htdocs/pub/repositories/' 
- - - example => '/srv/www/htdocs/pub/repositories/<mycustomrpms>/', put all rpms here.
- - Then create a channel for it to be attached to, the "Parrent" channel of this new channel should be the SUSE pool channel.
- - - So, its SUSE-Channel-Pool => Custom-Child-Channel => Custom-Repository- Like this example-
- - - - "spacecmd softwarechannel_listchildchannels sles12-sp3-pool-x86_64" => custom-rpms
- - Then, also in the WebUI, in System => Activation Keys => <your-key/s> => Child Channels Tick the box for the <Custom-RPMs> child-channel you created
- - This will automatically be cloned into your "Lifecycle-Management" channels and promoted accordingly.

Lifecycle-Management
- According to "SUSE Manager Best Practices" and "Advanced Patch Lifecycle Management with SUSE Manager" ... "SUSE provided channels should NEVER be assigned to managed systems. One reason is that you do not have control over when the content is updated and made available." [Chapter 5-1 of the Administration guide for SUSE Manager and is also in the 2 titles mentioned above.]
- - EXAMPLE => Patch Non-Production one week/month to test the patches prior to implementing to Production, and then patch the Production the following week/month => packages will be different and test in Non-Production will be invalid.

- There are 2 ways to accomplish this, 
- - 1 => Manually in WebUI with some manual CLI as well <= NOT Discussed in this writing
- - 2 => Use my script, or create your own, to automate this entire process.

- - My Automation Script
- - - Requirements-
- - - - Package => "spacewalk-utils" (in the repository for SUSE Manager)
- - - - - This will give you the "spacewalk-manage-channel-lifecycle" command used in my script.
- - - - - Upon the first run of the script it will check for this package and offer to install it for you if it not installed.
- - - - Source File for Local credentials and custom additions.
- - - - - Upon the first run of the script it will check for this file and offer to create it for you if it does not exist.

- - - Script Actions
- - - - First Run (Every run will check and verify)
- - - - - Check for a "credentials" file to be sourced, and offer to create it if not present.
- - - - - Source the "credentials" file
- - - - - Check for the "spacewalk-utils" package and offer to install it for you if it not installed.
- - - - - Check for the "~/.mgr-sync" file and create and populate it if not present
- - - - - Refresh and sync all SUSE Channel Trees using the "mgr-sync -s refresh" and the "spacewalk-repo-sync" commands.
- - - - - Check for the "dev-..." channel tree to determine first-run or not
- - - - - - Run the "spacewalk-manage-channel-lifecycle" command with the "--init" option in the first-run only, it uses the "--promote" thereafter.
- - - - - - - This will clone the SUSE Channel Tree/s into "dev-..." trees as in "sles12-sp3-pool-x86_64" => "dev-sles12-sp3-pool-x86_64" and all child channels accordingly.
- - - - - - If not present it will create an "Activation Key" for each "dev-..." pool.
- - - - - - If not present it will create a "Bootstrap Script" with it's activation key for each "dev-..." pool.
- - - - - Check for the "test-..." channel tree to determine first-run or not
- - - - - - Run the "spacewalk-manage-channel-lifecycle" command with the "--promote" option.
- - - - - - - This will Promote (clone) the "dev-..." Channel Trees into "test-..." as in "dev-sles12-sp3-pool-x86_64" => "test-sles12-sp3-pool-x86_64" and all child channels accordingly.
- - - - - - If not present it will create an "Activation Key" for each "test-..." pool.
- - - - - - If not present it will create a "Bootstrap Script" with it's activation key for each "test-..." pool.
- - - - - - - This will Promote (clone) the "test-..." Channel Trees into "prod-..." as in "test-sles12-sp3-pool-x86_64" => "prod-sles12-sp3-pool-x86_64" and all child channels accordingly.
- - - - - - If not present it will create an "Activation Key" for each "prod-..." pool.
- - - - - - If not present it will create a "Bootstrap Script" with it's activation key for each "prod-..." pool.
- - - - - Run any Custom functions added to the "credentials" file- NOTE => this function call needs to be manually created and called either in the "credentials" file or in the script itself.
- - - - - Email the named recipient with the results.
- - - - All logging is done in '~/reposync.log'
- - - - The script and the "credentials" file are located in '~/bin/'
- - - - The first-run will not run them out of sequence- they must be run i order- -b, -d, then -p

- - - - All runs after the first run will require no user interaction and can be put into cron-job/s.

APPENDIX A Lifecycle-Management
Here's how I manage mine-
- First Monday of every Month at 1am, 3am, 5am =>
- [-b], [-d], [-p]
- Every Monday at 1am =>
- - [-b] <= This channel is exclusively used for our PCI and other Compliance clients that MUST be kept patched at least Every Month with the Latest available packages.
- We patch Non-Production PCI and other Compliance clients on a Monday, and then their Production counterparts on the following Thursday, allowing 3 days for OS and Application/s testing.
- NON-Compliance clients are patched Quarterly, by groups, 1 group per month with the Non-Prod done in the first week of the month and the Production done in the third or fourth week of the month.

APPENDIX B More with Salt
Benefits of using Salt-Managed Clients
- Scheduling is accurate and on time, instantaneous or in correct time with the schedule.
- - Traditionally managed clients require the client scheduled "rhn_check" which is set default at 240 Minutes, even if scheduled in advance its not very dependable.

APPENDIX C PXE-Cobbler Configuration
- {no documentation here as of this revision, although I do have it working in production}

